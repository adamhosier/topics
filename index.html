<!--

NOTES FOR GROUP:

To draw a styled box with the bold header "Example":
  <div class="box" title="Example">
    ...
  </div>
NOTE: The sub-navigator will generate links based on every <div id="box"> element
      So if you had 3 boxes, with titles "eg1", "eg2", "eg3", there will be links
      in the sub-navigator to each of these three boxes

Write a paragraph of text:
  <p>Text goes here</p>

Add a reference to your text;
  <p>Some referece<sup id="p2"></sup></p>
'<sup>' is a superscript tag
'id="p2"' means its a reference to the probabilistic reference 2
          use p1, p2, p2 ... for probabilistic
              c1, c2, c3 ... for case based
              l1, l2, l3 ... for logic

Ordered list of refernces
  <ol>
    <li>Reference with id 1</li>
    <li>Another referece (id 2)</li>
    ...
  </ol>
First <li> corresponds to <sup id="p1"></sup>

See full code for examples :)


View the live website at:
https://rawgit.com/adamhosier/topics/master/index.html

-->

<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Intention Recognition - Techniques</title>
  <link rel="stylesheet" type="text/css" href="main.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
  <script type="text/javascript" src="main.js"></script>
</head>
<body>
  <div id="header">
      <h1>Intention Recognition</h1> <h2>Techniques</h2>
  </div>
  <div id="navigator">
    <div id="links">
      <a href="#introduction">Introduction</a>
      <a href="#probabilistic">Probabilistic</a>
      <a href="#casebased">Case Based</a>
      <a href="#logic">Logic</a>
    </div>
  </div>
  <div id="sub-navigator">
    <div id="sub-links">

    </div>
  </div>
  <div id="content">

    <!-- INTRODUCTION PAGE -->
    <div class="page" id="page-introduction">
      <div class="box" title="Introduction">
        <p>Intention recognition is the idea of analyzing the actions of an agent to predict what that agent's goal is. It can be described as the opposite of planning, where a goal is known and actions are planned to complete this goal. In intention recognition, the actions are known, and we must try to guess the goal of the agent.</p>
      </div>
      <div class="box" title="Early Work">
        <p>The topic was first researched in the 1980s, where it was originally intended to help with automatic response generation, for example in computer help messages. The field today is must more widespread, there is research being done in creating ambient intelligence, sophisticated computer games and in military applications such as terrorism detection.</p>
      </div>
      <div class="box" title="Overview">
        <p>There are usually three components to consider when creating an intention recognition system. Firstly a set of end goals, or intentions that can be reached. Secondly some type of knowledge about how these goals can be reached and lastly the sequence of actions performed by the agent. Combining these three ideas it would be possible to recognise the goal of an agent.</p>
        <p>There are three main formalisms of intention recognition that we will explore further in this topic. You can read more about the logic-based, cased-based and probabilistic approaches in their respective pages.</p>
      </div>
      <div class="box" title="Example">
        <img src="airport.png" width="500px" height="300px" align="right">
        <p align="left">In this website we demonstrate the different techniques with a real world example.</p>
        <p> Take a man in his 40s, acting nervously in an international airport. He has not checked in and is waiting in a crowded area with a single bag. He is sitting in the lower seating area, then is seen at the blue X rushing to the bathroom and leaving his bag at the position of the red X.</p>
        <p> In this website we will explore different ways that can we design a system to recognise and appropriately respond to behaviours such as this.</p>
      </div>
      <div class="box" title="Comparison">

      </div>
    </div>

    <!-- PROBABILISTIC PAGE -->
    <div class="page" id="page-probabilistic">
      <div class="box" title="Why Probabilistic Based Techniques Are Used">
        <p>The motivation behind producing a probabilistic approach for dealing with intention recognition becomes apparent when tackling the appropriate applications. Particularly in highly uncertain situations - using noisy sensors (Charniak & Goldman 1993, p. 64)<sup id="p3"></sup> - perhaps where a fast and immediate response is necessary (Kasteren & Englebienne & Kröse 2010, Chapter 8)<sup id="p4"></sup>, other models are less viable due to their lack of nuance and efﬁciency.</p>
        <p>As the name probabilistic implies, the aforementioned uncertain processes are consequently dealt with effectively, using probabilities. But what exactly substantiates an uncertain situation? Many factors have and can be posited. For instance, an agent (the subject for whom we are guessing the intent of) who is sentient, cannot have a fully accurate perception of their surroundings. Therefore, a rational, thinking agent cannot generally perform their own intentions in a simple if/then fashion as they themselves have to deal with their own intrinsic uncertainties, which are garnered from their incomplete knowledge of the environment (Tahboub 2005, p.4)<sup id="p1"></sup>.</p>
        <p>Also, simple knowledge of rational agents suggests that individual responses to environments will vary drastically due to variations in differing agents which could add uncertainty into recognising the possible intentions. This is creates intention complexity (Heinze 2004, p.25)<sup id="p5"></sup>. Note that this factor is more likely to contribute signiﬁcant amounts of uncertainty if the agent pool (the theoretical number of agents to be observed by the computerised recognition) is large and therefore a targeted approach - whereby speciﬁc agent/s characteristics are known and thus the agent/s possible reactions can be more accurately deﬁned against the general population - becomes less practical. Furthermore, in a large agent pool, either more time and resourses are needed to map the agent/s characteristics<sup id="p5"></sup>, if known, to more personalised intentions; alternatively, an agent/s speciﬁc characteristics can simply be ignored, adding uncertainty, but saving potential efﬁciency and time in a model implementation. Although these factors and decisions may seem speciﬁc to a situation and hard to envision, they are vital in choosing how to curate models for implementations which can vary all the way from airport security, to smart homes and speech recognition.</p>
      </div>

      <div class="box" title="How And What Are These Techniques Implemented">
        <p>One of most widely used and common model for interpreting probabilistic situations is the Hidden Markov Model (in which ‘hidden’ stands for the states unknown which for intention Recognition, is obviously the intentions). This model is a specialised Bayesian Network which is dynamic, meaning time is factor in the model and directly effects the current state in the model. For such a model you have three basic building blocks (Rabiner & Juang, January 1986, p. 6-7)<sup id="p2"></sup>.</p>
        <ul>
          <li>A ﬁnite number of states, say N. Dependant on the situation, states are fairly hard to come up with and must be limiting due to the inﬁnite complexity of most the potential applications<sup id="p2"></sup>. The balance between complexity and efﬁciency can involve a great degree of trial and error<sup id="p2"></sup>. These states can be represented diagrammatically, or for a ﬂexible computing implementation, a tree of linked lists with variable numbers of pointers with attached probability weighting (this is especially useful for initial experimentation in which trial and error like, heuristic thinking can alter the number of nodes needed). This can be implemented on most imperative programming languages.</li>
          <li>There is a clock time, say t. On a new clock time, dependant on the previous states probability distribution, a new state is entered. Therefore the states must change for an increase in clock time meaning time is treated as a distinct quantity - indeed everything in this model is. For these times, a sequence is used to keep track of the current states and observations (mentioned next). For a computerised implementation, an iterative solution can be employed where, as an example, the language iterates based on a object with a time ﬁeld (t1, t2 .. tX for each object one to X, time represented as an incrementing int), a state ﬁeld (s1, s2 .. sX) and an observation (o1, o2 .. oX). Furthermore, the current state (state ﬁeld) will for our applications of the HMM model, be ‘hidden’ and thus most likely null. Whilst implementing an algorithm to ﬁnd a probable ending state, or intention, these ﬁelds will change from null to a probability distribution based object.</li>
          <li>Finally, observation symbols can be produced based on the current states probability distribution for observation emissions, say M. For intention recognition purposes however, observations are found and then are used to approximate the appropriate state that led to them. Therefore, for the intention recognition use of HMM’s, each new distinct observation denotes a need for a new t (which will in turn increment) so for the iterator for generating a sequence, the probable implementation of hasNext() will be based on whether something new is observed about the agent. Observations will probably be implemented similar to state diagrams on a computer as described above on the same tree linked list system as the states.</li>
        </ul>
        <p>There is one additional component intrinsic to the model, the probabilities. In this model they are represented as arcs, which: from state to state, describe the probability at the next time t of the state changing to the state the arrowhead points at (it can point to itself); from state to observation, describing the probability at the next time t of the state exhibiting the observed at where the arrowhead points at. By basic mathematical probability theory, we can infer that all outgoing state to state arrows from a state must sum to probability 1. Similarly, all state to observed arrows from a state must also sum to probability 1. This whole system, before observations are made to start the time iteration, can be statically implemented on a computer system in a tree like structure as described previously.</p>
        <p>Hopefully, by showing some potential core underpinnings to the implementation, it is not just clear what the primary concepts of HMM’s are, but also how it is a relatively easy task to abstract the process to a computerised system with a degree of ﬂexibility. Still, a model is not useful unless it is used to make some predictions. After exploring the algorithms for this, I will then give an example to hopefully clarify the whole process further.</p>
      </div>

      <div class="box" title="Algorithms To Find Intentions">
        <p>There are many algorithms commonly used to ﬁnd various pieces of information about the changing sequence of states N, over time t, given an sequence of observations M. The main algorithms are, the forward algorithm (for a process called ﬁltering), the forward-backward algorithm (forward can be combined with backward for a process called smoothing) and the Viterbi algorithm (used to ﬁnd the likelihood of a state sequence). Filtering uses the forward algorithm to ﬁnd a probability distribution of what state the sequence is currently at. Smoothing uses the same forward algorithm to also ﬁnd a probability distribution of what state the sequence is currently at. However, there is a subtle difference as smoothing looks at what state the sequence was in at a past point in time t, not the most recent time - essentially meaning that information gained later in the sequence of observations can be used to give a more precise prediction of where the states preceding the extra observations were. This retrospective linking of observations is why the word ‘backward’ is used, to imply information gained from the future is used to trace back the steps that led there increasing the accuracy of a prediction compared to one without the extra observations. Finally, the Viterbi algorithm is a process that doesn’t only predict a state at a time t, but the whole sequence of states over a range of times.</p>
        <p>Based on the context of Intention Recognition, the most appropriate algorithms to employ are generally the forward and forward-backward, algorithms. For most contexts, especially when constantly monitoring and updating the intentions of an agent, the sequence that led to a current state is insigniﬁcant compared to the probability of a state. We just want to know which process is most likely and respond to it if required, the transitions are just a way of getting to these states. For most uses on intelligent agents, every new observation would, after being added to the sequence, require a re-computation of the forward algorithm on this new state in time and if necessary a forward-backwards algorithm for ﬁnding potential intentions in important places earlier on with the extra information gained.</p>
        <p>The forward algorithm is deﬁned as follows (Wikipedia mathematics use)<sup id="p6"></sup>:</p>
        <p>We are trying to ﬁnd probability <span>p(x<sub>t</sub>, y<sub>1:t</sub>)</span>. <span>x<sub>t</sub></span> is the probability of state x at time t given all observations y up to time t (1 to t). If we want to calculate the entire probability distribution, we would do this calculation over all states X. The summation of said distribution should then, if the algorithm is performed correctly sum to 1. To efﬁciently compute the probability above, we make use of the rules for conditional independence in the HMM which are fairly simple due to only having dependence for directly preceding states. Using a combination of chain rules and model conditions this expression simpliﬁes to:</p>
        <span style="font-size:16px">α<sub>t</sub>(x<sub>t</sub>) = p(y<sub>t</sub>|x<sub>t</sub>)Σ<sub>x<sub>t-1</sub></sub>p(x<sub>t</sub>|x<sub>t-1</sub>)α<sub>t-1</sub>(x<sub>t-1</sub>)</span>
        <p>Where <span>α<sub>t</sub>(x<sub>t</sub>)</span> is  <span>p(x<sub>t</sub>, y<sub>1:t</sub>)</span>. The probability of <span>p(y<sub>t</sub>|x<sub>t</sub>)</span> is simply the state to the observed probability arc as described earlier. The probability of <span>p(x<sub>t</sub>|x<sub>t-1</sub>)</span> is simply the transition probability and we evidently recurse on <br><span>α<sub>t-1</sub>(x<sub>t-1</sub>)</span>.</p>
        <p>To compute this algorithm efﬁciently without unnecessary recalculations, it makes sense to do this iteratively using the sequence of time and observed as stated in the implementation techniques heading above. Then, when we have a new observation, we don’t have to do a new calculation as we use the previous ones. In that case we start from a ﬁrst, start state probability 1 that eventually feeds into the others. Hence <span>α<sub>t</sub>(x<sub>t</sub>)</span> uses the previous iteration <span>α<sub>t-1</sub>(x<sub>t-1</sub>)</span> and the transition and emission (state to observed) arcs <span>p(x<sub>t</sub>|x<sub>t-1</sub>)</span> and <span>p(y<sub>t</sub>|x<sub>t</sub>)</span> respectfully to get the next iteration of the algorithm.</p>
        <p>The Backward Algorithm works in a similar way but essentially in reverse and merges both calculations to calculate a probability distribution in a past time t. It might be sensible to use the forward-backward algorithm if at some point the highest probability states were similar but after future observations it might clear up which was more likely by factoring these extra observations in - especially if one of these close, second or third highest probability intentions required immediate action from the program and were considered important intentions.</p>
      </div>

      <div class="box" title="How To Estimate The Probability Arcs">
        <p>Creating the states for a particular HMM can require a great deal of trial and error as stated earlier (Rabiner & Juang, January 1986, p. 6-7)<sup id="p2"></sup>, but also hard is forming estimations for probability arcs. Helpfully however, there are algorithms to help estimate the probabilities between state to state transitions called parameter estimations. One widely used algorithm is the Baum-Welch. It iteratively updates the probabilities in a log-likeliness way but is in no way always optimal (Tu, p.1)<sup id="p7"></sup>.</p>
      </div>

      <div class="box" title="Response To Intentions">
        <p>After knowing the likelihood of an agent having an intention using the forward or forward-backward algorithms described, the probability distribution can be further weighted in terms of how necessary it is to deal with the intention. For instance, if applied to airport security - if a man leaves his bag (observation) and then runs to the toilet (observation) and the resultant probabilities that the mans intention is to detonate a bomb in his bag, or he forgot his bag are equal, it makes sense to prepare for the worst and respond to a bomb threat as that is far more serious. Therefore the intentions should be weighted, in an airport safety manner, in terms of risk on a multiplication based scale. The highest number should then indicate what action is most required.</p>
      </div>

      <div class="box" title="Conclusion">
        <p>To summarise. Probabilistic approaches to dealing with intention recognition are a viable and computationally achievable way to predict and react to an agent. Through establishing state diagrams and their observation probabilities, combined with algorithms to calculate probable intentions based on sensor data, it is very possible to respond to various situational environments with a high degree of information presented as probabilities.</p>
      </div>

      <div class="box" title="References">
        <ol>
          <li>KARIM A. TAHBOUB. 2005. Intelligent Human–Machine Interaction Based on Dynamic Bayesian Networks Probabilistic Intention Recognition. Journal of Intelligent and Robotic Systems. Palestine Polytechnic University.</li>
          <li>L. R. Rabiner & B. H. Juang. 1986. An Introduction to Hidden Markov Models. IEEE ASSP Magazine. Stanford.</li>
          <li>E. Charniak and R.P. Goldman. 1993. A Bayesian model of plan recognition. Artificial Intelligence.</li>
          <li>T.L.M. van Kasteren, G. Englebienne, and B.J.A. Kröse.Human Activity Recognition from Wireless Sensor Network Data: Benchmark and Software. Intelligent Systems Lab Amsterdam, Science Park 107, 1098 XG, Amsterdam, The Netherlands.</li>
          <li>Clint Heinze. 2004. Modelling Intention Recognition for Intelligent Agent Systems. DSTO Systems Sciences Laboratory, Edinburgh, South Australia, Australia.</li>
          <li><a href=" http://en.wikipedia.org/wiki/Forward_algorithm">http://en.wikipedia.org/wiki/Forward_algorithm</a></li>
          <li>Stephen Tu. Unknown date. Derivation of Baum-Welch Algorithm for Hidden Markov Models. University of Berkeley.​</li>
        </ol>
      </div>
    </div>

    <!-- CASE BASED PAGE -->
    <div class="page" id="page-casebased">
      <div class="box" title="Case Based">
        <p>Lorem ipsum<sup id="c1"></sup> dolor sit amet, consectetur adipiscing elit. Maecenas odio dui, euismod ac lacus elementum, feugiat bibendum leo. Pellentesque dolor leo, iaculis quis dolor in, rutrum pellentesque mauris. Proin quis hendrerit arcu. Aenean molestie nulla sit amet tellus porttitor, fermentum fringilla mi imperdiet. Donec at risus ac mauris tempus vulputate nec in massa. Curabitur blandit vitae sem in aliquam. Sed ultrices metus urna, id pharetra mauris rhoncus et. Quisque vitae dignissim diam. Integer luctus elit et posuere tempus. Vivamus posuere est non mauris pellentesque ultricies. Aliquam erat volutpat. Suspendisse elit tellus, rhoncus quis tempus ut, bibendum quis quam. Donec dignissim, tortor at luctus rhoncus, magna felis sollicitudin massa, in ultricies libero lacus eu magna. Sed eros felis, pharetra in tincidunt id, eleifend sed lectus. Nunc et ligula in ante dignissim molestie. Sed efficitur facilisis libero, id lobortis tellus scelerisque ac.</p>
        <p>Nam sollicitudin<sup id="c2"></sup> urna ut mauris maximus, vitae volutpat orci iaculis. Mauris dapibus at urna id bibendum. Etiam augue mauris, iaculis dignissim semper quis, pharetra mollis enim. Cras vitae mi a dolor interdum fermentum porta id libero. Aliquam erat volutpat. Duis augue odio, facilisis ac felis in, porttitor ullamcorper purus. Maecenas vel est nisl. Vestibulum a sem molestie, posuere urna eget, finibus mi. Nulla sit amet ligula finibus, auctor nunc a, ullamcorper ante. Praesent dapibus porttitor dolor, tempor ultricies ipsum fringilla sed. Nullam fringilla pellentesque convallis. Sed eu viverra ligula.</p>
        <p>Nulla ac neque vitae dolor pretium accumsan vel sit amet risus. Aenean eget sagittis neque. Mauris eu maximus augue, at sollicitudin lectus. Aliquam porta sollicitudin massa in mattis. Proin ultricies urna nulla, vitae sollicitudin neque maximus condimentum. Nunc tempor at dui sed pellentesque. Praesent nulla purus, auctor auctor odio sit amet, congue sodales diam. Nulla facilisi. Morbi quis eros at metus convallis auctor quis eu odio. Proin vestibulum turpis quis viverra suscipit. Pellentesque tincidunt felis vel turpis accumsan, nec convallis diam egestas. Suspendisse potenti. Aliquam id luctus metus.</p>
      </div>

      <div class="box" title="References">
        <ol>
          <li>Write</li>
          <li>some</li>
          <li>references</li>
        </ol>
      </div>
    </div>

    <!-- LOGIC PAGE -->
    <div class="page" id="page-logic">
      <div class="box" title="Logic">
        <p>Lorem ipsum<sup id="l1"></sup> dolor sit amet, consectetur adipiscing elit. Maecenas odio dui, euismod ac lacus elementum, feugiat bibendum leo. Pellentesque dolor leo, iaculis quis dolor in, rutrum pellentesque mauris. Proin quis hendrerit arcu. Aenean molestie nulla sit amet tellus porttitor, fermentum fringilla mi imperdiet. Donec at risus ac mauris tempus vulputate nec in massa. Curabitur blandit vitae sem in aliquam. Sed ultrices metus urna, id pharetra mauris rhoncus et. Quisque vitae dignissim diam. Integer luctus elit et posuere tempus. Vivamus posuere est non mauris pellentesque ultricies. Aliquam erat volutpat. Suspendisse elit tellus, rhoncus quis tempus ut, bibendum quis quam. Donec dignissim, tortor at luctus rhoncus, magna felis sollicitudin massa, in ultricies libero lacus eu magna. Sed eros felis, pharetra in tincidunt id, eleifend sed lectus. Nunc et ligula in ante dignissim molestie. Sed efficitur facilisis libero, id lobortis tellus scelerisque ac.</p>
        <p>Nam sollicitudin urna ut<sup id="l2"></sup> mauris maximus, vitae volutpat orci iaculis. Mauris dapibus at urna id bibendum. Etiam augue mauris, iaculis dignissim semper quis, pharetra mollis enim. Cras vitae mi a dolor interdum fermentum porta id libero. Aliquam erat volutpat. Duis augue odio, facilisis ac felis in, porttitor ullamcorper purus. Maecenas vel est nisl. Vestibulum a sem molestie, posuere urna eget, finibus mi. Nulla sit amet ligula finibus, auctor nunc a, ullamcorper ante. Praesent dapibus porttitor dolor, tempor ultricies ipsum fringilla sed. Nullam fringilla pellentesque convallis. Sed eu viverra ligula.</p>
        <p>Nulla ac neque vitae dolor pretium accumsan vel sit amet risus. Aenean eget sagittis neque. Mauris eu maximus augue, at sollicitudin lectus. Aliquam porta sollicitudin massa in mattis. Proin ultricies urna nulla, vitae sollicitudin neque maximus condimentum. Nunc tempor at dui sed pellentesque. Praesent nulla purus, auctor auctor odio sit amet, congue sodales diam. Nulla facilisi. Morbi quis eros at metus convallis auctor quis eu odio. Proin vestibulum turpis quis viverra suscipit. Pellentesque tincidunt felis vel turpis accumsan, nec convallis diam egestas. Suspendisse potenti. Aliquam id luctus metus.</p>
      </div>

      <div class="box" title="References">
        <ol>
          <li>Logic</li>
          <li>references</li>
          <li>go</li>
          <li>here</li>
        </ol>
      </div>
    </div>

    <div id="footer">
      <span>Adam Hosier</span>
      <span>2015</span>
    </div>
  </div>
</body>
</html>
